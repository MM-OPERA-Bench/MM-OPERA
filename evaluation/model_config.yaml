# MM-OPERA/evaluation/model_config.yaml

# --- API Provider Configurations ---
# Stores base URLs and how to get API keys for different providers
api_providers:
  aigptx: # Example provider name
    base_url: "https://api.ohmygpt.com/v1"
    # base_url: "https://www.aigptx.top/v1"
    # Store API key in an environment variable
    api_key_env_var: "AIGPTX_API_KEY" # Name of the environment variable


# --- Model Specific Configurations ---
# Lists models and their specific settings
# Ensure your model to judge and judge model is defined in the 'models' section
models:
  GPT-4-Omni: # model name (DO NOT include whitespaces; it will be used in the results file name)
    model_identifier: "gpt-4o"
    provider: "aigptx" # Must match a key in 'api_providers'
    endpoint: "/chat/completions" # Appended to the provider's base_url
  
  Gemini-2.0-Flash-Thinking-Exp:
    model_identifier: "gemini-2.0-flash-thinking-exp-01-21"
    provider: "aigptx"
    endpoint: "/chat/completions"

  GPT-4o-judge:
    model_identifier: "gpt-4o-2024-08-06"
    provider: "aigptx"
    endpoint: "/chat/completions"

  # Add other models you plan to support

# --- Evaluation Task Specific Settings ---
evaluation_settings:
  ria:
    default_model_name: "Gemini-2.0-Flash-Thinking-Exp" # Must match a key in 'models'
    default_max_tokens: 4096 # Specific to RIA task if not overridden by model
    num_images_to_process: 2
    sleep_time_after_judge_api: 1 # seconds
    prompt: |
      Describe each image briefly.
      Analyze and explore the relation between the two images, identifying any possible connections, themes, or shared elements.
      Formulate the output as follows:

      - First image: [image concept]
      - Second image: [image concept]
      - Relation: [one keyword, phrase or sentence]
      - Explanation: [1-5 sentences]
    

    regular_judge:
      default_judge_model_name: "GPT-4o-judge"  # can be overridden by command-line arg
      default_max_tokens_per_item_judge: 2048 # Per item in batch
      default_max_tokens_judge: 20000  # max tokens for the judge's response
      judge_group_size: 1 # How many items to send to the judge in one API call
      # Sleep time after each judge API call
      sleep_time_after_judge_api: 3 # seconds
      prompt: |
        Evaluate the quality of responses provided by MLLMs using the following criteria:
          - **4 points**: Accurate and logically consistent. Convey the same level of thoughtfulness and insight, the same caliber of intellectual rigor and comprehension with reference answer. (Not required to be literally identical to reference answers)
          - **3 points**: Shows reasonable understanding but may be incomplete or lack key insights, such as mentioning key facts but failing to effectively summarize the relationship. However, what can be directly got from the images or a common theme with little similarity with the reference answer are not key facts.
          - **2 points**: Somewhat relevant but lacks depth, is overly broad, or misses critical elements needed for a coherent explanation; does not address or align with the intended standard reasoning at all.
          - **1 point**: Displays uncertainty, offering vague or incomplete reasoning, or concedes with phrases like "I don't know."
          - **0 points**: Factuality hallucination in the explanation, i.e. contains factual inaccuracies or fabricated information that undermines its reasoning.

          You will receive multiple independent questions in a numbered format:
          1. MLLM Output:[] \n reference answer: [] \n\n 2. MLLM Output:[] \n reference answer: [] \n\n

          Provide your response in JSON format where each key is the question number and the value is your answer:
          {
              "1": {"score_4o": "",\n"score_reason": ""},
              "2": {"score_4o": "",\n"score_reason": ""}
          }

          **Rating examples:**

          **Example 1**
          MLLM Output:["- **First image**: Chicago skyline with Willis Tower and glass buildings.\n- **Second image**: Cloud Gate ("The Bean") sculpture in Millennium Park.\n- **Relation**: **Chicago landmarks**\n- **Explanation**: Both are iconic Chicago structures - the Willis Tower defining the skyline and Cloud Gate serving as a renowned public sculpture, representing the city's architectural heritage."] \n reference answer: ["relation": "Chicago","explanation": "Both Skyscraper and Bean are located in Chicago. Skyscrapers are a prominent feature of Chicago's skyline, and Cloud Gate (\"The Bean\") is a famous public sculpture located in Millennium Park, Chicago.","reasoning": "LocatedIn(Skyscraper, Chicago)\nLocatedIn(Bean, Chicago)\nThus, Skyscraper → Chicago and Bean → Chicago"]\n
          {"score_4o": "4",\n"score_reason": "The MLLM's output captures the essence of both images as notable Chicago landmarks, consistent with the reference answer, although its summary 'Chicago landmarks' differs from the reference answer 'Chicago.'"}

          **Example 2**
          MLLM Output:["- **First image**: Crown of thorns - Christian symbol of suffering.\n- **Second image**: Portrait of Bach, composer of religious music.\n- **Relation**: Religious devotion and expression.\n- **Explanation**: The crown represents Christ's Passion while Bach composed spiritual works like St. Matthew Passion, both expressing Christian themes of suffering and devotion."] \n reference answer: ["relation": "St. Matthew Passion", "explanation": "The crown of thorns represents the Passion of Christ, the subject of the work. The portrait of Bach directly connects to the composer of the St. Matthew Passion.","reasoning": "Depict(StMatthewPassion, PassionOfChrist) and Symbolize(CrownOfThorns, PassionOfChrist)\nCompose(Bach, StMatthewPassion)\nThus, CrownOfThorns → PassionOfChrist → StMatthewPassion and Bach → StMatthewPassion"]\n
          {"score_4o": "3",\n"score_reason": "The MLLM's output partially captures the relationship but lacks distinctiveness. However, it correctly mentions Bach's work 'St. Matthew Passion' which is a key fact consistent with the reference answer."}

          **Example 3**
          MLLM Output:["- **First image**: Crystal chandelier.\n- **Second image**: White masquerade mask.\n- **Relation**: Elegance and mystery.\n- **Explanation**: The chandelier suggests opulence in theater settings, while the mask represents anonymity at formal events, combining themes of luxury and hidden identities."] \n reference answer: ["relation": "The Phantom Of the Opera","explanation": "The white mask is the Phantom's signature disguise, concealing his disfigured face and adding to his mystique. The falling chandelier is a dramatic and iconic scene from the musical, representing the Phantom's power and anger.","reasoning": "Wear(Phantom, Mask) and Character(Phantom, ThePhantomOfTheOpera)\nEvent(FallingChandelier, ThePhantomOfTheOpera)\nThus, Mask → Phantom → ThePhantomOfTheOpera and FallingChandelier → ThePhantomOfTheOpera",]\n
          {"score_4o": "2",\n"score_reason": "The concept 'Elegance and mystery' is too broad and it fails to mention any key facts related to 'The Phantom of the Opera'."}

          **Example 4**
          MLLM Output:["- **First image**: Man in formal attire\n- **Second image**: Plover near water \n- **Relation**: Unknown connection  \n- **Explanation**: No clear connection between the formal portrait and shorebird. Different subjects without obvious relationship."] \n reference answer: ["relation": "the origin of the Guinness World Records","explanation": "The Guinness World Records was created by Sir Hugh Beaver, the managing director of the Guinness Brewery. The idea for the book came about when he got into an argument about which was the fastest game bird in Europe, the golden plover or the red grouse, while on a hunting trip.","reasoning": "Create(GuinnessWorldRecords, SirHughBeaver)\nIdeaCameFrom(GuinnessWorldRecords, GoldenPlover)\nThus, SirHughBeaver → GuinnessWorldRecords → GoldenPlover",]\n
          {"score_4o": "1",\n"score_reason": "The MLLM expresses uncertainty with 'Unknown connection.'"}

          **Example 5**
          MLLM Output:["- **First image**: Sanmao comic book cover  \n- **Second image**: Sahara Desert map  \n- **Relation**: Journey  \n- **Explanation**: Sanmao's adventures include stories in the Sahara Desert, connecting the character's travels with the desert setting."] \n reference answer: ["relation": "三毛","explanation": "三毛 is a Taiwanese writer who wrote 'The Stories of the Sahara'. The story is about her life in the Sahara Desert. The Sahara map is related to the story. Sanmao is a manhua character created by Zhang Leping in 1935. They share the same name.","reasoning": "WriteBy(三毛, TheStoriesOfSahara) and Related(TheStoriesOfSahara, SaharaMap)\nSameName(三毛, Sanmao)\nThus, SaharaMap → TheStoriesOfSahara → 三毛 → Sanmao"]\n
          {"score_4o": "0",\n"score_reason": "The MLLM includes incorrect facts, confusing 'Sanmao' with unrelated adventures in the Sahara Desert. It has confused the facts related to 'Sanmao (comics)' with those of 'Sanmao (writer).'"}

  # --- Judge Configurations ---
    reasoning_judge:
      default_judge_model_name: "GPT-4o-judge"  # can be overridden by command-line arg
      judge_max_tokens_per_batch_multiplier: 2048 # Per item in batch
      judge_group_size: 1 # How many items to send to the judge in one API call
      # Sleep time after each judge API call
      sleep_time_after_judge_api: 3 # seconds
      prompt: |
        You are an expert judge evaluating association paths between two image concepts. Your task:
        1. Analysis Input
          input: {
            concepts: [ImageA_desc, ImageB_desc],
            reference_answer: {
              relation: string,
              explanation: string,
              path: string
            },
            mllm_output: {
              description: [ImageA_desc, ImageB_desc],
              relation: string,
              explanation: string
            }
          }
        2. Path Standardization for mllm_output
          rules: {
            type1_sequential: "Predicate_{11}(A, X_{11}) and ... and Predicate_{1i}(X_{1i}, X) and Predicate_{21}(X, X_{21}) and ... and Predicate_{2j}(X_{2j}, B)\nA → X_{11} → ... → X_{1i} → X → X_{21} → ... → X_{2j} → B"
            type2_convergent: "Predicate_{11}(A, X_{11}) and ... and Predicate_{1i}(X_{1i}, X)\nPredicate_{21}(B, X_{21}) and ... and Predicate_{2j}(X_{2j}, X)\nA → X_{11} → ... X_{1i} → X and B → X_{21} → ... → X_{2j} → X" 
            type3_metaphorical: "A ∧ B → X"
            format: {
              - PascalCase for entities/predicates
              - 'and' for clause connection
              - '∧' for entity connection
              - '→' for each association hop
            }
          }
        3. Quality Assessment
          for each hop in path:
            score: [
              float[0-1], // Reasonableness: logical soundness to the ideal relation X
              float[0-1], // Distinctiveness: The clarity of concept boundaries
              int[0 or 1]  // Knowledgeability: Domain knowledge depth
            ]
            Note: The path don't need to be identical to the reference answer but should demonstrate equivalent quality of reasoning.

        You will receive multiple independent problem-answer pairs in a numbered format:
        1.\nProblem: ...
        2.\nProblem: ...
        Provide your response in JSON format where each key is the problem-answer pair number and the value is your evaluation:
        {"1": {...},"2": {...}}

        Examples 1:
        Problem:
        - First image: Bread and Wine
        - Second image: 12
        Reference Answer:
        - Relation: The Last Supper
        - Explanation: The Last Supper is a religious painting depicting the final meal Jesus shared with his disciples before his crucifixion. The bread and wine represent the elements of the Eucharist, a central sacrament in Christianity. The number 12 refers to the twelve apostles present at the Last Supper.
        - Association Path: Depict(TheLastSupper, Eucharist) and Represent(BreadAndWine, Eucharist)\nNumber(TheLastSupper, 12)\nThus, BreadAndWine → Eucharist → TheLastSupper and 12 → TheLastSupper
        MLLM's Output:
        - First image: A loaf of bread and a chalice
        - Second image: The number 12 in gold
        - Relation: Christian symbolism
        - Explanation: The first image is a visual representation of the Eucharist, a central sacrament in Christianity, where bread and wine are consecrated and consumed as the body and blood of Jesus Christ. The number 12 holds significant symbolism in Christianity, representing the 12 apostles of Jesus. Together, these images allude to a key aspect of Christian faith and tradition. 
        Your evaluation:
        {
        "path": "Represent(BreadAndWine, Eucharist) and Symbolize(Eucharist, Christian)\nNumber(12, ApostlesOfJesus) and Related(ApostlesOfJesus, Christian)\nThus, BreadAndWine → Eucharist → Christian and 12 → ApostlesOfJesus → Christian", 
        "hop_quality": {
        "BreadAndWine → Eucharist": [1.00, 1.00, 1],
        "Eucharist → Christian": [1.00, 1.00, 1],
        "12 → ApostlesOfJesus": [1.00, 1.00, 1],
        "ApostlesOfJesus → Christian": [1.00, 1.00, 1]
        }
        "explanation": "The MLLM constructed a well-structured convergent path through Christian symbolism. Both paths (BreadAndWine→Eucharist→Christian and 12→ApostlesOfJesus→Christian) demonstrate perfect scores across all dimensions, showing deep theological understanding and precise use of religious concepts."
        }

        Examples 2:
        Problem:
        - First image: Michelangelo
        - Second image: ceiling
        Reference Answer:
        - Relation: Sistine Chapel ceiling
        - Explanation: Michelangelo painted the Sistine Chapel ceiling, one of the most famous works of art in the world.
        - Association Path: PaintedBy(Michelangelo, SistineChapelCeiling)\nRelatedTo(SistineChapelCeiling, ceiling)\nThus, Michelangelo → SistineChapelCeiling → ceiling
        MLLM's Output:
        - First image: A portrait of Michelangelo.
        - Second image: A white, textured ceiling.
        - Relation: Artistic influence.
        - Explanation: Michelangelo was a renowned artist known for his intricate details and mastery of form. The ceiling pattern in the second image, with its grid-like design and raised lines, evokes the style and detail often seen in Michelangelo's artwork. Although not directly referencing a specific piece, the ceiling's texture suggests an artistic inspiration from a master like Michelangelo, reflecting a respect for classical art and the lasting influence it continues to have on contemporary design.
        Your evaluation:
        {
        "path": "Represent(Michelangelo, ArtisticInfluence)\nEvoke(Ceiling, MichelangeloArtwork) and Suggest(MichelangeloArtwork, ArtisticInfluence)\nThus, Michelangelo → ArtisticInfluence and Ceiling → MichelangeloArtwork → ArtisticInfluence", 
        "hop_quality": {
        "Michelangelo → ArtisticInfluence": [0.95, 0.18, 1],
        "Ceiling → MichelangeloArtwork": [0.07, 0.34, 1],
        "MichelangeloArtwork → ArtisticInfluence": [0.96, 0.16, 0],
        }, 
        "explanation": "While logically sound, the MLLM's path relies on general artistic influence rather than identifying the iconic Sistine Chapel connection. The first hop maintains high reasonableness but lacks specificity, while subsequent hops show diminishing quality due to vague artistic associations."
        }

        Examples 3:
        Problem:
        - First image: bread with butter
        - Second image: a falling cat
        Reference Answer:
        - Relation: Buttered cat paradox
        - Explanation: The Buttered Cat Paradox is a humorous thought experiment that involves a cat with buttered toast strapped to its back.
        - Association Path: Butter ∧ Cat → ButteredCatParadox
        MLLM's Output:
        - First image: A slice of bread with butter spread on it, next to a knife and a piece of butter on a table.
        - Second image: A cat in mid-air, appearing to be jumping or falling against a white background.
        - Relation: Contrast between stillness and movement.
        - Explanation: The first image depicts a static scene of preparing food, while the second image captures a dynamic moment of a cat in motion. The contrast highlights the difference between the calmness of everyday tasks and the energy of an animal's movement.
        Your evaluation:
        {
        "path": "PreparingFood ∧ CatInMotion → ContrastBetweenStillnessAndMovement", 
        "hop_quality": {
        "PreparingFood ∧ CatInMotion → ContrastBetweenStillnessAndMovement": [0.78, 0.14, 0],
        }, 
        "explanation": "The MLLM constructed a superficial static-dynamic contrast, missing the cultural reference to the 'Buttered Cat Paradox'. While the observation is logically reasonable, it lacks both distinctiveness and knowledge depth by failing to recognize the underlying thought experiment."
        }


  ica:
    default_model_name: "Gemini-2.0-Flash-Thinking-Exp"
    default_max_tokens: 4096
    rotate_test: true
    num_images_to_process: 3
    sleep_time_after_judge_api: 1
    prompt: |
      1. Briefly describe **Image 1**, **Image 2**, and **Image 3** based on their visual information.
      2. Analyze the relationship between **Image 1** and **Image 2**, identifying any possible connections, themes, or shared elements that link **Image 1** to **Image 2**.
      3. Design **Image 4** so that its relationship with **Image 3** mirrors that between **Image 1** and **Image 2**. Use insights from the first pair to guide your design.
      4. Present your analysis in this format:
      - **Image 1**: [image concept]
      - **Image 2**: [image concept]
      - **Image 3**: [image concept]
      - **Image 4**: [image concept that you create]
      - **Relation**: [a keyword, phrase, or sentence describing the connection]
      - **Explanation**: [1-5 sentences detailing the reasoning and its application to both pairs]
    
    regular_judge:
      default_judge_model_name: "GPT-4o-judge"  # can be overridden by command-line arg
      expect_rotated_test_results: true
      default_max_tokens_per_item_judge: 2048 # Per item in batch
      default_max_tokens_judge: 20000  # max tokens for the judge's response
      judge_group_size: 1 # How many items to send to the judge in one API call
      # Sleep time after each judge API call
      sleep_time_after_judge_api: 3 # seconds
      prompt: |
        Background: MLLMs analyze three images, identify relationships between Images 1 and 2, and propose Image 4 that maintains the same relationship with Image 3.

        Your task: Evaluate MLLM responses focusing on Image 4 description, Relation, and Explanation against reference answers using these criteria:

        - **4 points**: Accurate and logically consistent. Correctly matches reference answer's logic (not necessarily identical). Accurate relation identification and logical Image 4 description.
        - **3 points**: Generally correct but incomplete. Shows reasonable understanding but may be incomplete or lack key insights, such as mentioning key facts but failing to effectively summarize the relationship.
        - **2 points**: Somewhat reasonable but lacks depth, is overly broad, or misses critical elements needed for a coherent explanation; does not address or align with the intended standard reasoning at all; completely unrelated to the reference answer's focus; completely misses the thematic connection; does not address the intended theme; a completely different interpretation. Vague relation or loose connection between image 3 and 4.
        - **1 point**: Responds with 'I don't know,' 'I can't help,' 'Unknown connection,' etc., or expresses its uncertainty cautiously.
        - **0 points**: Contains factual errors or illogical content. Factuality hallucination in the explanation, i.e. contains factual inaccuracies or fabricated information that undermines its reasoning. (Note that hallucination is only related to the MLLM's explanation. That is, even if MLLM's answer does not address or align with the intended standard reasoning at all, it must not get 0 points unless it refers to erroneous information. Divergent opinions on the image theme do not lead to factual hallucination, but wrong image content does.)

        You will receive multiple independent questions in a numbered format:
        1. MLLM Output:[] \n reference answer: [] \n\n 2. MLLM Output:[] \n reference answer: [] \n\n

        Provide your response in JSON format where each key is the question number and the value is your answer:
        {
            "1": {"score_4o": "",\n"score_reason": ""},
            "2": {"score_4o": "",\n"score_reason": ""}
        }

        **Rating examples**:

        **Example 1**
        MLLM Output:["- **Image 1**: film Amélie\n- **Image 2**: Eiffel Tower\n- **Image 3**: From Vegas to Macau II\n- **Image 4**: Macau Tower\n- **Relation**: Setting/Location\n- **Explanation**: Image 1 (Amélie) is closely tied to Paris, France, where the film was set and shot. Image 2 shows the Eiffel Tower, a symbol of Paris. Likewise, Image 3 (From Vegas to Macau II) highlights Macau in both its title and visuals. Thus, Image 4 should feature a notable Macau landmark, such as the Macau Tower.\n"] \n reference answer: ["Image 4": "the new Lisboa Hotel", "relation": "Films Associated with Iconic Locations", "explanation": "Amélie is a film that captures the essence of Paris, showcasing its charm, streets, and culture. Similarly, From Vegas to Macau (澳门风云) is centered around the gambling and nightlife culture in Macau, with the New Lisboa Hotel being a significant landmark in that context.", "reasoning path 1": "FilmSetting(Amelie, Paris)\nCulturalSymbol(EiffelTower, Paris)\nThus, Amelie → Paris", "reasoning path 2": "FilmSetting(MacauStorm, Macau)\nCulturalSymbol(NewLisboaHotel, Macau)\nThus, MacauStorm → NewLisboaHotel"]
        {"score_4o": "4","score_reason": "The MLLM accurately linked 'Amélie' with Paris via the Eiffel Tower and 'From Vegas to Macau II' with Macau via the Macau Tower. Although the reference answer highlighted the New Lisboa Hotel for Macau, the Macau Tower is also a valid and recognizable symbol. The MLLM's explanation was clear and logical, with no errors."}

        **Example 2**
        MLLM Output:["- **Image 1**: Flour spilling with wheat stalks nearby.\n- **Image 2**: Red tulips.\n- **Image 3**: Crosswalk scene.\n- **Image 4**: Musical notes.\n- **Relation**: Homophones\n- **Explanation**: The connection between Image 1 (flour) and Image 2 (flower) is based on their similar pronunciation. To mirror this, Image 4 should relate to Image 3 using a homophone: 'band' (a group of musicians) and 'banned' (prohibited), both pronounced similarly. So, Image 4 features a musical theme to play on the word 'band.'"] \n reference answer: ["Image 4": "Beetles", "relation": "Homophones: flaʊə and bi:təls", "explanation": "The word 'Flower' sounds like 'Flour.' Similarly, 'Beetles' sounds like 'Beatles,' referencing the famous band.", "reasoning path 1": "Homophone(Flower, Flour)\nThus, Flower → Flour", "reasoning path 2": "Homophone(Beetle, TheBeatles)\nThus, Beetle → TheBeatles"]
        {"score_4o": "3","score_reason": "The MLLM correctly identified the homophone relationship between 'flour' and 'flower' but used 'band' and 'banned' instead of 'beetles' and 'Beatles,' which are more culturally relevant. The explanation was logical but missed the specific cultural reference, scoring 3 out of 4 for being mostly correct but not fully aligned."}

        **Example 3**
        MLLM Output:["- **Image 1**: Burj Khalifa.\n- **Image 2**: Canton Tower.\n- **Image 3**: Statue of Liberty.\n- **Image 4**: Christ the Redeemer.\n- **Relation**: Iconic landmarks and their architectural counterparts in other countries. \n- **Explanation**: Image 1 and Image 2 both showcase famous towers known for their unique architecture. Applying this relationship to Image 3 (Statue of Liberty in New York City), Image 4 should depict a similarly iconic large-scale statue in a different country. Christ the Redeemer in Rio de Janeiro fits this description.\n"] \n reference answer: ["Image 4": "Eiffel Tower", "relation": "Built in the Same Year", "explanation": "The Burj Khalifa in Dubai and Canton Tower in Guangzhou were both completed in 2010, marking a significant year for architectural achievements. Similarly, the Statue of Liberty and the Eiffel Tower were both inaugurated in 1886, symbolizing important cultural landmarks in their respective countries.", "reasoning path 1": "BuiltIn(BurjKhalifa, 2010)\nBuiltIn(CantonTower, 2010)\nThus, BurjKhalifa → 2010 and CantonTower → 2010", "reasoning path 2": "BuiltIn(StatueOfLiberty, 1886)\nBuiltIn(EiffelTower, 1886)\nThus, StatueOfLiberty → 1886 and EiffelTower → 1886"]
        {"score_4o": "2","score_reason": "The MLLM correctly identified iconic landmarks but missed the focus on the construction year as the key relation. While it linked Christ the Redeemer and the Statue of Liberty based on cultural and architectural significance, this did not match the standard's emphasis on their shared construction year. Scoring 2 for being partially correct but lacking the critical facts."}

        **Example 4**
        MLLM Output:["I'm uncertain about the connection, but here's what I observe:\n- **Image 1**: A modern peaker.\n- **Image 2**: An antique gramophone.\n- **Image 3**: Basketball hoop.\n- **Image 4**: Unknown concept, possibly related to sports.\n- **Relation**: Unknown connection\n- **Explanation**: The relationship between these images isn't clear to me. They might represent different themes or concepts."] \n reference answer: ["Image 4": "A peach basket", "relation": "Evolution of Equipment", "explanation": "The phonograph was an early device for playing recorded sound, which evolved into modern audio equipment like speakers. Similarly, basketball was originally played using a peach basket as a goal. Over time, this evolved into the modern basketball hoop with a net and backboard.", "reasoning path 1": "OriginalEquipment(Phonograph, Speaker)\nThus, Phonograph → Speaker", "reasoning path 2": "OriginalEquipment(PeachBasket, BasketballHoop)\nThus, PeachBasket → BasketballHoop"]
        {"score_4o": "1", "score_reason": "The MLLM expresses uncertainty and is unable to identify the specific connection between the images. It correctly describes Images 1, 2, and 3 but is unsure about Image 4."}

        **Example 5**
        MLLM Output:["- **Image 1**: "Amélie," featuring a woman with short black hair and a playful expression.\n- **Image 2**: Eiffel Tower.\n- **Image 3**: A movie poster suggesting an action or drama film with a glamorous setting.\n- **Image 4**: The Great Wall surrounded by lush jungle.\n- **Relation**: Setting\n- **Explanation**: Image 1 and Image 2 both highlight Paris, with "Amélie" set in the city and the Eiffel Tower as its iconic symbol. For Image 3, the Asian film taking place in Macao setting naturally leads to Image 4, showcasing the Great Wall. It perfectly captures the essence of an Asian landmark, and the lush jungle enhances its exotic appeal, aligning seamlessly with the theme."] \n reference answer: ["Image 4": "the new Lisboa Hotel", "relation": "Films Associated with Iconic Locations", "explanation": "Amélie is a film that captures the essence of Paris, showcasing its charm, streets, and culture. Similarly, From Vegas to Macau (澳门风云) is centered around the gambling and nightlife culture in Macau, with the New Lisboa Hotel being a significant landmark in that context.", "reasoning path 1": "FilmSetting(Amelie, Paris)\nCulturalSymbol(EiffelTower, Paris)\nThus, Amelie → Paris", "reasoning path 2": "FilmSetting(MacauStorm, Macau)\nCulturalSymbol(NewLisboaHotel, Macau)\nThus, MacauStorm → NewLisboaHotel"]
        {"score_4o": "0", "score_reason": "The MLLM included factually incorrect information, stating the Great Wall is in Macao and surrounded by lush jungle. The Great Wall is in mainland China, not Macao."}

    reasoning_judge:
      default_judge_model_name: "GPT-4o-judge"  # can be overridden by command-line arg
      judge_max_tokens_per_batch_multiplier: 2048 # Per item in batch
      judge_group_size: 1 # How many items to send to the judge in one API call
      # Sleep time after each judge API call
      sleep_time_after_judge_api: 3 # seconds
      prompt: |
        MLLMs analyze three images, identify relationships between Images 1 and 2 (Pair 1), and propose Image 4 that maintains the same relationship with Image 3 (Pair 2).
        You are an expert judge evaluating MLLM's association path. Your task:
        1. Analysis Input
          input: {
            concepts: [Image1_desc, Image2_desc, Image3_desc],
            reference_answer: {
              image4: Image4_desc
              relation: string,
              explanation: string,
              path: string
            },
            mllm_output: {
              description: [Image1_desc, Image2_desc, Image3_desc, Image4_desc],
              relation: string,
              explanation: string
            }
          }
        2. Path Standardization for mllm_output (2 paths)
          rules: {
            type1_sequential: "Predicate_{11}(A, X_{11}) and ... and Predicate_{1i}(X_{1i}, X) and Predicate_{21}(X, X_{21}) and ... and Predicate_{2j}(X_{2j}, B)\nA → X_{11} → ... → X_{1i} → X → X_{21} → ... → X_{2j} → B"
            type2_convergent: "Predicate_{11}(A, X_{11}) and ... and Predicate_{1i}(X_{1i}, X)\nPredicate_{21}(B, X_{21}) and ... and Predicate_{2j}(X_{2j}, X)\nA → X_{11} → ... X_{1i} → X and B → X_{21} → ... → X_{2j} → X" 
            type3_metaphorical: "A ∧ B → X"
            format: {
              - PascalCase for entities/predicates
              - 'and' for clause connection
              - '∧' for entity connection
              - '→' for each association hop
            }
          }
          If the mllm_output didn't provide a feasible response, provide empty strings as the paths.
        3. Quality Assessment for mllm_output
            for each path in the two paths:
              for each hop in path:
                score: [
                  float[0-1], // Reasonableness: logical soundness to the ideal relation X
                  float[0-1], // Distinctiveness: The clarity of concept boundaries
                  int[0 or 1]  // Knowledgeability: Domain knowledge depth
                ]
            Note: The path don't need to be identical to the reference answer but should demonstrate equivalent quality of reasoning.
            If the mllm_output is irrelevant or incorrect, provide a low score for each hop.

        You will receive multiple independent problem-answer pairs in a numbered format:
        1. Problem: ...
        2. Problem: ...
        Provide your response in JSON format where each key is the problem-answer pair number and the value is your evaluation:
        {"1": {...},"2": {...}}

        Examples 1:
        Problem:
        - Image 1: Burj Khalifa
        - Image 2: Canton Tower
        - Image 3: Statue of Liberty
        Reference Answer:
        - Image 4: Eiffel Tower
        - Relation: Built in the Same Year
        - Explanation 1: The Burj Khalifa in Dubai and Canton Tower in Guangzhou were both completed in 2010, marking a significant year for architectural achievements.
        - Association Path 1: BuiltIn(BurjKhalifa,2010)\nBuiltIn(CantonTower,2010) \n Thus, BurjKhalifa → 2010 and CantonTower → 2010
        - Explanation 2: The Statue of Liberty and the Eiffel Tower were both inaugurated in 1886, symbolizing important cultural landmarks in their respective countries.
        - Association Path 2: BuiltIn(StatueOfLiberty,1886) \nBuiltIn( EiffelTower, 2010)\n Thus, StatueOfLiberty → 1886 and EiffelTower → 1886
        MLLM's Output:
        - **Image 1**: The Burj Khalifa, the tallest building in the world, located in Dubai, United Arab Emirates.
        - **Image 2**: Canton Tower, also the tallest tower in China, located in Guangzhou, Guangdong Province, China.
        - **Image 3**: The Statue of Liberty, a colossal neoclassical sculpture on Liberty Island in New York Harbor, New York City.
        - **Image 4**: The Eiffel Tower, a wrought iron lattice tower on the Champ de Mars in Paris, France.
        - **Relation**: \"Tallest Structures\"
        - **Explanation**: All four images depict some of the tallest structures in the world, each representing a different architectural style and cultural significance. Image 1 and Image 2 are skyscrapers, while Image 3 is a statue, and Image 4 is an iconic tower. This common theme allows for a comparison of how these structures have become symbols of their respective cities and countries.
        Your evaluation:
        {
        "path1": "Is(BurjKhalifa, Tallest)\nIs(CantonTower, Tallest)\nBurjKhalifa → Tallest and CantonTower → Tallest", 
        "path2": "Is(StatueOfLiberty, Tallest)\nIs(EiffelTower, Tallest)\nStatueOfLiberty → Tallest and EiffelTower → Tallest", 
        "hop_quality_path1": {
        "BurjKhalifa → Tallest": [0.95, 0.86, 1],
        "CantonTower → Tallest": [0.95, 0.85, 1]
        },
        "hop_quality_path2": {
        "StatueOfLiberty → Tallest": [0.55, 0.45, 1],
        "EiffelTower → Tallest": [0.83, 0.85, 1]
        },
        "explanation": "In the MLLM's output, the first path shows consistently high hop quality scores (0.85-0.95) as both the Burj Khalifa and Canton Tower are indeed among the tallest structures. The second path shows more varied scores, with the Statue of Liberty receiving lower scores (0.45-0.78) as it's not typically categorized among the world's tallest structures, while the Eiffel Tower maintains high scores (0.83-0.89)."
        }

        Examples 2:
        Problem:
        - Image 1: C Major
        - Image 2: See
        - Image 3: G Major
        Reference Answer:
        - Image 4: Glee
        - Relation: Homophones related to musical notes
        - Explanation 1: The pronunciation of the letter 'C' sounds exactly like the word 'See'
        - Association Path 1: Homophone(C, See)\nThus, C → See
        - Explanation 2: The pronunciation of 'G' is homophonous with the word 'Glee,' which signifies joy or happiness.
        - Association Path 2: Homophone(G, Glee)\nThus, G → Glee
        MLLM's Output:
        - **Image 1**: A piano keyboard illustration highlighting the C Major chord.
        - **Image 2**: A person holding a magnifying glass up to their eye, creating a humorous enlarged eye effect.
        - **Image 3**: Musical notation for the G Major scale.
        - **Image 4**: A humorous image of a magnifying glass enlarging musical notes or sheet music.
        - **Relation**: Emphasis and Clarity
        - **Explanation**: Image 1 emphasizes specific notes clearly within a piano chord, while Image 2 uses a magnifying glass to humorously emphasize and enlarge a part of a person. Image 4 mirrors this by using a magnifying glass to emphasize specific musical notes from Image 3, establishing a connection through the theme of emphasis and clarity in both musical and visual elements.
        Your evaluation:
        {
        "path1": "PianoKeyboard ∧ MagnifyingGlass → EmphasisAndClarity", 
        "path2": "MusicalNotation ∧ MagnifyingGlass → EmphasisAndClarity", 
        "hop_quality_path1": {
        "PianoKeyboard ∧ MagnifyingGlass → EmphasisAndClarity": [0.09, 0.05, 0]
        },
        "hop_quality_path2": {
        "MusicalNotation ∧ MagnifyingGlass → EmphasisAndClarity": [0.07, 0.04, 0]
        },
        "explanation": "The MLLM's interpretation significantly deviates from the intended homophone relationship in the reference answer, despite maintaining structural consistency between the two pairs, resulting in low scores."
        }


# --- General Settings ---
general_settings:
  huggingface_cache_dir: "dataset"
  huggingface_dataset_name: "titic/MM-OPERA"
  
  # Base directories for outputs, relative to project root
  results_base_dir: "results"
  logs_base_dir: "logs"

  # Default sleep time between API requests (in seconds)
  sleep_time_between_requests: 1